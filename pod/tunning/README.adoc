:data-uri:
:toc: left
:markup-in-source: +verbatim,+quotes,+specialcharacters
:source-highlighter: rouge
:icons: font
:stylesdir: stylesheets
:stylesheet: colony.css

= Pod Tunning

.Goals
* Understand K8s Pod tunning

:sectnums:

== Pod traffic shapping
Limiting Pod ingress or egress network traffic.

.References :
* https://v1-16.docs.kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/#support-traffic-shaping[]
* https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-configuring.html#nodes-pods-configuring-bandwidth_nodes-pods-configuring[]

[source,yaml]
----
kind: Pod
apiVersion: v1
metadata:
  name: iperf-slow
  annotations:
    kubernetes.io/egress-bandwidth: 10M <1>
    kubernetes.io/ingress-bandwidth: 10M <2>
spec:
  containers:
    - name: hello-openshift
      image: openshift/hello-openshift
----
<1> limit the egress
<2> limit the ingress

== Pod Disruptions

.References :
* https://v1-16.docs.kubernetes.io/docs/concepts/workloads/pods/disruptions/[]
* https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-configuring.html#nodes-pods-configuring-pod-distruption-about_nodes-pods-configuring[]

A PDB limits the number of pods of a replicated application that are down simultaneously from https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions[voluntary disruptions].
For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum. A PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have.
For example, a Deployment which has a .spec.replicas: 5 is supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the Eviction API will allow voluntary disruption of one, but not two pods, at a time.

An example in OCP 4 is "etcd-quorum-guard" which ensure that maximum we can afford the loss of one pod

[source,bash]
----
oc get pdb/etcd-quorum-guard -n openshift-machine-config-operator -o yaml
----
[source,yaml]
----
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: etcd-quorum-guard
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: etcd-quorum-guard
----

The group of pods that comprise the application is specified using a label selector, the same as the one used by the application’s controller (deployment, stateful-set, etc).
The “intended” number of pods is computed from the .spec.replicas of the pods controller. The controller is discovered from the pods using the .metadata.ownerReferences of the object.

Pods which are deleted or unavailable due to a rolling upgrade to an application do count against the disruption budget,
but controllers (like deployment and stateful-set) are not limited by PDBs when doing rolling upgrades – the handling of failures during application updates is configured in the controller spec.


== Pod Priority and Preemption
Pods can have priority. Priority indicates the importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible.
.References :
https://v1-16.docs.kubernetes.io/docs/concepts/configuration/pod-priority-preemption/[]
https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-configuring.html#nodes-pods-configuring-critical_nodes-pods-configuring[]

=== PriorityClass
A PriorityClass is a non-namespaced object that defines a mapping from a priority class name to the integer value of the priority.
The name is specified in the name field of the PriorityClass object’s metadata. The value is specified in the required value field. The higher the value, the higher the priority.

==== Effect of Pod priority on scheduling order
Scheduler orders pending Pods by their priority and a pending Pod is placed ahead of other pending Pods with lower priority in the scheduling queue.
As a result, the higher priority Pod may be scheduled sooner than Pods with lower priority if its scheduling requirements are met.
If such Pod cannot be scheduled, scheduler will continue and tries to schedule other lower priority Pods.

[source,bash]
----
oc get priorityclass
NAME                      VALUE        GLOBAL-DEFAULT   AGE
system-cluster-critical   2000000000   false            4h36m <1>
system-node-critical      2000001000   false            4h36m <2>
----
<1> priority class for pods that are important to the cluster but can be removed if necessary.
<2> priority class for pods that should never be evicted from a node.

* https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-priority.html#admin-guide-priority-preemption-priority-class_nodes-pods-priority[]

[source,yaml]
----
apiVersion: scheduling.k8s.io/v1
description: Used for system critical pods that must not be moved from their current
  node.
kind: PriorityClass
metadata:
  name: system-node-critical
value: 2000001000
----

==== PriorityClass optional fields

===== globalDefault
The globalDefault field indicates that the value of this PriorityClass should be used for Pods without a priorityClassName.
Only one PriorityClass with globalDefault set to true can exist in the system.
If there is no PriorityClass with globalDefault set, the priority of Pods with no priorityClassName is zero

===== preemptionPolicy
What is preemption? When Pods are created, they go to a queue and wait to be scheduled.
The scheduler picks a Pod from the queue and tries to schedule it on a Node.
If no Node is found that satisfies all the specified requirements of the Pod, preemption logic is triggered for the pending Pod.
Let’s call the pending Pod P. Preemption logic tries to find a Node where removal of one or more Pods with lower priority than P would enable P
to be scheduled on that Node. If such a Node is found, one or more lower priority Pods get evicted from the Node. After the Pods are gone, P can be scheduled on the Node.

. PodDisruptionBudget is supported, but not guaranteed!

. Inter-Pod affinity on lower-priority Pods
A Node is considered for preemption only when the answer to this question is yes: “If all the Pods with lower priority than the pending Pod are removed from the Node, can the pending Pod be scheduled on the Node?”

Note: Preemption does not necessarily remove all lower-priority Pods. If the pending Pod can be scheduled by removing fewer than all lower-priority Pods, then only a portion of the lower-priority Pods are removed. Even so, the answer to the preceding question must be yes. If the answer is no, the Node is not considered for preemption.
If a pending Pod has inter-pod affinity to one or more of the lower-priority Pods on the Node, the inter-Pod affinity rule cannot be satisfied in the absence of those lower-priority Pods. In this case, the scheduler does not preempt any Pods on the Node. Instead, it looks for another Node. The scheduler might find a suitable Node or it might not. There is no guarantee that the pending Pod can be scheduled.

Our recommended solution for this problem is to create inter-Pod affinity only towards equal or higher priority Pods.
